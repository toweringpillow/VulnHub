name: Scrape World News

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch: # Allow manual triggering

# Ensure workflow runs even if previous runs failed
concurrency:
  group: world-news-cron
  cancel-in-progress: false

jobs:
  scrape-world-news:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Check Schedule Status
        run: |
          echo "üïê Current UTC time: $(date -u)"
          echo "üìÖ Scheduled to run every 6 hours"
          echo "üîç Workflow triggered by: ${{ github.event_name }}"
      
      - name: Trigger World News Scraper
        run: |
          # Construct URL - handle both with and without https://
          URL="${{ vars.SCRAPER_URL }}"
          
          # Debug: Check if variable exists
          echo "üîç Checking for SCRAPER_URL variable..."
          
          if [[ -z "$URL" ]]; then
            echo "‚ùå ERROR: SCRAPER_URL variable is empty or not set!"
            echo "üí° Make sure you:"
            echo "   1. Go to Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí Variables tab"
            echo "   2. Created a REPOSITORY variable (not environment variable) named SCRAPER_URL"
            echo "   3. Set the value to: www.vulnerabilityhub.com (use www to avoid redirects)"
            exit 1
          fi
          
          # Normalize URL: trim whitespace and remove trailing slashes
          URL=$(echo "$URL" | xargs | sed 's|/$||')
          
          # Extract domain and construct base URL
          if [[ "$URL" =~ ^https?:// ]]; then
            # URL already has protocol, extract domain
            DOMAIN=$(echo "$URL" | sed -E 's|^https?://||' | cut -d'/' -f1)
            # Get base URL (protocol + domain, no path)
            BASE_URL=$(echo "$URL" | sed -E 's|^(https?://[^/]+).*|\1|')
          else
            # URL doesn't have protocol, extract domain and add https://
            DOMAIN=$(echo "$URL" | cut -d'/' -f1)
            BASE_URL="https://${DOMAIN}"
          fi
          
          # Construct full endpoint URL
          FULL_URL="${BASE_URL}/api/cron/world-news"
          
          echo "‚úÖ Extracted domain: ${DOMAIN}"
          echo "‚úÖ Base URL: ${BASE_URL}"
          echo "‚úÖ Full URL: ${FULL_URL}"
          
          # Make the request with error handling
          echo "üì° Making request to world news scraper endpoint..."
          echo "üì° URL: ${FULL_URL}"
          
          set +e  # Don't exit on error immediately
          response=$(curl --location-trusted -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            -H "Content-Type: application/json" \
            -w "\nHTTP_CODE:%{http_code}" \
            --connect-timeout 10 \
            --max-time 30 \
            -v \
            "${FULL_URL}" 2>&1)
          CURL_EXIT=$?
          set -e
          
          if [ $CURL_EXIT -ne 0 ]; then
            echo "‚ùå curl failed with exit code: $CURL_EXIT"
            echo "Full curl output:"
            echo "$response"
            
            # Check if it's a timeout error (exit code 28)
            if [ $CURL_EXIT -eq 28 ]; then
              echo ""
              echo "‚ö†Ô∏è  Timeout error detected!"
              echo "üí° This might be due to:"
              echo "   1. Vercel platform timeout limits (check your plan)"
              echo "   2. Scraper taking longer than expected"
              echo "   3. Network issues"
              echo ""
              echo "The scraper may still be processing in the background."
              echo "Check your Vercel logs to see if the scrape completed."
            fi
            
            exit 1
          fi
          
          # Extract HTTP code and body
          http_code=$(echo "$response" | grep -o "HTTP_CODE:[0-9]*" | cut -d: -f2 || echo "000")
          body=$(echo "$response" | grep -v "^<" | grep -v "^>" | grep -v "HTTP_CODE:" | tail -n +1)
          
          echo ""
          echo "=== Response Summary ==="
          echo "HTTP Code: ${http_code}"
          echo "Response Body:"
          echo "$body"
          echo "========================"
          
          # Check if request was successful
          # Accept both 200 (completed) and 202 (accepted/processing) as success
          if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
            if [ "$http_code" -eq 202 ]; then
              echo "‚úÖ World news scraper accepted and processing in background (HTTP 202)"
              echo "üí° The scraper will continue running on the server even though this workflow completed"
            else
              echo "‚úÖ World news scraper triggered successfully (HTTP $http_code)"
            fi
          else
            echo "‚ùå World news scraper failed with HTTP $http_code"
            echo "Full response:"
            echo "$response"
            exit 1
          fi

