name: Scrape Articles

on:
  schedule:
    # Run every 5 minutes
    - cron: '*/5 * * * *'
  workflow_dispatch: # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger Scraper
        run: |
          # Construct URL - handle both with and without https://
          URL="${{ vars.SCRAPER_URL }}"
          
          # Debug: Show what we received
          echo "üîç SCRAPER_URL variable: ${URL}"
          
          if [[ -z "$URL" ]]; then
            echo "‚ùå ERROR: SCRAPER_URL variable is empty or not set!"
            exit 1
          fi
          
          if [[ ! "$URL" =~ ^https?:// ]]; then
            URL="https://$URL"
          fi
          
          echo "‚úÖ Full URL: ${URL}/api/cron/scraper"
          
          # Make the request with error handling
          response=$(curl -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            -H "Content-Type: application/json" \
            -w "\nHTTP_CODE:%{http_code}" \
            -s \
            "${URL}/api/cron/scraper") || {
            echo "‚ùå curl failed with exit code: $?"
            exit 1
          }
          
          # Extract HTTP code and body
          http_code=$(echo "$response" | grep -o "HTTP_CODE:[0-9]*" | cut -d: -f2)
          body=$(echo "$response" | sed '/HTTP_CODE:/d')
          
          echo "Response:"
          echo "$body"
          
          # Check if request was successful
          if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
            echo "‚úÖ Scraper triggered successfully (HTTP $http_code)"
          else
            echo "‚ùå Scraper failed with HTTP $http_code"
            exit 1
          fi

