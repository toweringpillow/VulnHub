name: Scrape Articles

on:
  schedule:
    # Run every 5 minutes
    - cron: '*/5 * * * *'
  workflow_dispatch: # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger Scraper
        run: |
          # Construct URL - handle both with and without https://
          URL="${{ vars.SCRAPER_URL }}"
          
          # Debug: Show what we received
          echo "üîç SCRAPER_URL variable: ${URL}"
          
          if [[ -z "$URL" ]]; then
            echo "‚ùå ERROR: SCRAPER_URL variable is empty or not set!"
            exit 1
          fi
          
          # Extract domain for DNS check
          if [[ "$URL" =~ ^https?:// ]]; then
            DOMAIN=$(echo "$URL" | sed -E 's|^https?://||' | cut -d'/' -f1)
            URL="$URL"
          else
            DOMAIN=$(echo "$URL" | cut -d'/' -f1)
            URL="https://$URL"
          fi
          
          echo "‚úÖ Full URL: ${URL}/api/cron/scraper"
          echo "üîç Testing DNS resolution for: ${DOMAIN}"
          
          # Test DNS resolution
          if ! host "${DOMAIN}" > /dev/null 2>&1; then
            echo "‚ùå DNS resolution failed for ${DOMAIN}"
            echo "üí° This usually means:"
            echo "   1. DNS hasn't fully propagated yet (can take 24-48 hours)"
            echo "   2. Domain isn't pointing to Vercel correctly"
            echo "   3. Domain is incorrect in SCRAPER_URL variable"
            echo ""
            echo "üîç Trying alternative DNS lookup..."
            nslookup "${DOMAIN}" || true
            exit 1
          fi
          
          echo "‚úÖ DNS resolution successful"
          
          # Make the request with error handling
          echo "üì° Making request to scraper endpoint..."
          response=$(curl -v -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            -H "Content-Type: application/json" \
            -w "\nHTTP_CODE:%{http_code}" \
            -s \
            "${URL}/api/cron/scraper" 2>&1) || {
            EXIT_CODE=$?
            echo "‚ùå curl failed with exit code: $EXIT_CODE"
            echo "Response output:"
            echo "$response"
            exit 1
          }
          
          # Extract HTTP code and body
          http_code=$(echo "$response" | grep -o "HTTP_CODE:[0-9]*" | cut -d: -f2)
          body=$(echo "$response" | sed '/HTTP_CODE:/d')
          
          echo "Response:"
          echo "$body"
          
          # Check if request was successful
          if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
            echo "‚úÖ Scraper triggered successfully (HTTP $http_code)"
          else
            echo "‚ùå Scraper failed with HTTP $http_code"
            exit 1
          fi

