name: Scrape Articles

on:
  schedule:
    # Run every 30 minutes to stay within GitHub Actions free tier (2,000 min/month)
    # Free tier: 2,000 minutes/month
    # At 30 min intervals: ~1,440 runs/month √ó ~1.5 min/run = ~2,160 min/month (slightly over, but close)
    # At 30 min intervals: ~1,440 runs/month √ó ~1 min/run = ~1,440 min/month (within limit)
    # Note: GitHub Actions may delay scheduled workflows during high load
    # Workflows run on the default branch (main) only
    - cron: '*/30 * * * *'
  workflow_dispatch: # Allow manual triggering

# Ensure workflow runs even if previous runs failed
# This prevents GitHub from skipping scheduled runs due to failures
concurrency:
  group: scraper-cron
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Check Schedule Status
        run: |
          echo "üïê Current UTC time: $(date -u)"
          echo "üìÖ Scheduled to run every 30 minutes"
          echo "üîç Workflow triggered by: ${{ github.event_name }}"
      
      - name: Trigger Scraper
        run: |
          # Construct URL - handle both with and without https://
          URL="${{ vars.SCRAPER_URL }}"
          
          # Debug: Check if variable exists
          echo "üîç Checking for SCRAPER_URL variable..."
          
          # Debug: Show what we received (with quotes to see whitespace)
          echo "üîç SCRAPER_URL variable (raw): '${URL}'"
          echo "üîç SCRAPER_URL variable length: ${#URL} characters"
          
          if [[ -z "$URL" ]]; then
            echo "‚ùå ERROR: SCRAPER_URL variable is empty or not set!"
            echo "üí° Make sure you:"
            echo "   1. Go to Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí Variables tab"
            echo "   2. Created a REPOSITORY variable (not environment variable) named SCRAPER_URL"
            echo "   3. Set the value to: www.vulnerabilityhub.com (use www to avoid redirects)"
            exit 1
          fi
          
          # Normalize URL: trim whitespace and remove trailing slashes
          URL=$(echo "$URL" | xargs | sed 's|/$||')
          
          # Extract domain and construct base URL
          if [[ "$URL" =~ ^https?:// ]]; then
            # URL already has protocol, extract domain
            DOMAIN=$(echo "$URL" | sed -E 's|^https?://||' | cut -d'/' -f1)
            # Get base URL (protocol + domain, no path)
            BASE_URL=$(echo "$URL" | sed -E 's|^(https?://[^/]+).*|\1|')
          else
            # URL doesn't have protocol, extract domain and add https://
            DOMAIN=$(echo "$URL" | cut -d'/' -f1)
            BASE_URL="https://${DOMAIN}"
          fi
          
          # Construct full endpoint URL
          FULL_URL="${BASE_URL}/api/cron/scraper"
          
          echo "‚úÖ Extracted domain: ${DOMAIN}"
          echo "‚úÖ Base URL: ${BASE_URL}"
          echo "‚úÖ Full URL: ${FULL_URL}"
          
          # Test DNS resolution with multiple methods
          echo "üîç Testing DNS resolution for: ${DOMAIN}"
          echo "--- Using 'host' command ---"
          host "${DOMAIN}" || echo "host command failed"
          echo "--- Using 'nslookup' command ---"
          nslookup "${DOMAIN}" || echo "nslookup command failed"
          echo "--- Using 'dig' command ---"
          dig "${DOMAIN}" +short || echo "dig command failed"
          
          # Try to resolve
          if ! getent hosts "${DOMAIN}" > /dev/null 2>&1; then
            echo "‚ùå DNS resolution failed for ${DOMAIN}"
            echo "üí° This usually means:"
            echo "   1. Domain is incorrect: '${DOMAIN}'"
            echo "   2. Network issue from GitHub Actions"
            echo ""
            echo "üîç Attempting curl anyway to see detailed error..."
          else
            echo "‚úÖ DNS resolution successful"
          fi
          
          # Make the request with error handling
          echo "üì° Making request to scraper endpoint..."
          echo "üì° URL: ${FULL_URL}"
          
          set +e  # Don't exit on error immediately
          # Use --location-trusted to forward Authorization header on redirects
          # (safe here since we control both domains)
          response=$(curl --location-trusted -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            -H "Content-Type: application/json" \
            -w "\nHTTP_CODE:%{http_code}" \
            --connect-timeout 10 \
            --max-time 30 \
            -v \
            "${FULL_URL}" 2>&1)
          CURL_EXIT=$?
          set -e
          
          if [ $CURL_EXIT -ne 0 ]; then
            echo "‚ùå curl failed with exit code: $CURL_EXIT"
            echo "Full curl output:"
            echo "$response"
            
            # Check if it's a timeout error (exit code 28)
            if [ $CURL_EXIT -eq 28 ]; then
              echo ""
              echo "‚ö†Ô∏è  Timeout error detected (90 seconds) - this is a known Vercel platform limit"
              echo "üí° The scraper endpoint is designed to return immediately, but Vercel may have"
              echo "   a 90-second proxy timeout that can't be overridden on free/hobby plans."
              echo ""
              echo "‚úÖ The scraper is likely still processing in the background on Vercel."
              echo "   Check your Vercel function logs to verify the scrape completed."
              echo ""
              echo "üìù This is expected behavior - the workflow will 'fail' but scraping continues."
              echo "   Consider this a success if you see scrape logs in Vercel."
              # Don't exit with error code - treat timeout as acceptable since scraping continues
              exit 0
            fi
            
            exit 1
          fi
          
          # Extract HTTP code and body (curl verbose output is on stderr, response on stdout)
          http_code=$(echo "$response" | grep -o "HTTP_CODE:[0-9]*" | cut -d: -f2 || echo "000")
          body=$(echo "$response" | grep -v "^<" | grep -v "^>" | grep -v "HTTP_CODE:" | tail -n +1)
          
          echo ""
          echo "=== Response Summary ==="
          echo "HTTP Code: ${http_code}"
          echo "Response Body:"
          echo "$body"
          echo "========================"
          
          # Check if request was successful
          # Accept both 200 (completed) and 202 (accepted/processing) as success
          if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
            if [ "$http_code" -eq 202 ]; then
              echo "‚úÖ Scraper accepted and processing in background (HTTP 202)"
              echo "üí° The scraper will continue running on the server even though this workflow completed"
            else
              echo "‚úÖ Scraper triggered successfully (HTTP $http_code)"
            fi
          else
            echo "‚ùå Scraper failed with HTTP $http_code"
            echo "Full response:"
            echo "$response"
            exit 1
          fi

